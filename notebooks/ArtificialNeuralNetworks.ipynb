{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks for Regression\n",
    "\n",
    "_By: Ling Li Ya, Liana_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/liana/.local/lib/python3.8/site-packages (2.7.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (12.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/lib/python3/dist-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (4.0.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (1.21.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /home/liana/.local/lib/python3.8/site-packages (from tensorflow) (0.22.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/liana/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.3.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/liana/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/liana/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/liana/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (59.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/liana/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/liana/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/liana/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.6->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/liana/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/liana/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/liana/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/liana/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: keras in /home/liana/.local/lib/python3.8/site-packages (2.7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 13:16:43.912527: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-02 13:16:43.912635: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned dataset with no outliers\n",
    "X_train_no = pd.read_csv('../dataset/no_outliers/X_train.csv')\n",
    "X_test_no = pd.read_csv('../dataset/no_outliers/X_test.csv')\n",
    "y_train_no = pd.read_csv('../dataset/no_outliers/y_train.csv')\n",
    "y_test_no = pd.read_csv('../dataset/no_outliers/y_test.csv')\n",
    "\n",
    "# Uncleaned original data\n",
    "X_train_with = pd.read_csv('../dataset/with_outliers/X_train.csv')\n",
    "X_test_with = pd.read_csv('../dataset/with_outliers/X_test.csv')\n",
    "y_train_with = pd.read_csv('../dataset/with_outliers/y_train.csv')\n",
    "y_test_with = pd.read_csv('../dataset/with_outliers/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def standardise_data(X_train, y_train, X_test, y_test):\n",
    "    # X_scaler = StandardScaler()\n",
    "    # y_scaler = StandardScaler()\n",
    "\n",
    "    # # Scale data to standardise data values\n",
    "    # X_train_trans = X_scaler.fit_transform(X_train)\n",
    "    # y_train_trans = y_scaler.fit_transform(y_train)\n",
    "    # X_test_trans = X_scaler.transform(X_test)\n",
    "    # y_test_trans = y_scaler.transform(y_test)\n",
    "    \n",
    "    # print(X_train_trans.shape, y_train_trans.shape, X_test_trans.shape, y_test_trans.shape)\n",
    "\n",
    "    # return X_scaler, y_scaler, X_train_trans, y_train_trans, X_test_trans, y_test_trans\n",
    "    # return X_scaler, y_scaler, X_train, y_train, X_test, y_test\n",
    "\n",
    "# X_scaler_no, y_scaler_no, X_train_no_trans, y_train_no_trans, X_test_no_trans, y_test_no_trans = standardise_data(X_train_no, y_train_no, X_test_no, y_test_no)\n",
    "X_train_no_trans, y_train_no_trans, X_test_no_trans, y_test_no_trans = X_train_no, y_train_no, X_test_no, y_test_no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Using a layer with 5 neurons, with 9 input dimensions of normal weight, using relu activation function. Batch size is 20 and training epochs are 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "29/29 [==============================] - 0s 908us/step - loss: 0.3062 - mse: 0.3062 - mae: 0.5059\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 0s 860us/step - loss: 0.2504 - mse: 0.2504 - mae: 0.4478\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 0s 879us/step - loss: 0.1528 - mse: 0.1528 - mae: 0.3321\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 0s 895us/step - loss: 0.0585 - mse: 0.0585 - mae: 0.1976\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 0s 885us/step - loss: 0.0387 - mse: 0.0387 - mae: 0.1588\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0351 - mse: 0.0351 - mae: 0.1511\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0311 - mse: 0.0311 - mae: 0.1422\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0266 - mse: 0.0266 - mae: 0.1315\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0224 - mse: 0.0224 - mae: 0.1204\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.1100\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0164 - mse: 0.0164 - mae: 0.1008\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0148 - mse: 0.0148 - mae: 0.0951\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0138 - mse: 0.0138 - mae: 0.0913\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0132 - mse: 0.0132 - mae: 0.0889\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0128 - mse: 0.0128 - mae: 0.0874\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0122 - mse: 0.0122 - mae: 0.0849\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0118 - mse: 0.0118 - mae: 0.0835\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0115 - mse: 0.0115 - mae: 0.0823\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0113 - mse: 0.0113 - mae: 0.0811\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0112 - mse: 0.0112 - mae: 0.0811\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0110 - mse: 0.0110 - mae: 0.0802\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0108 - mse: 0.0108 - mae: 0.0790\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0107 - mse: 0.0107 - mae: 0.0790\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0107 - mse: 0.0107 - mae: 0.0787\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0106 - mse: 0.0106 - mae: 0.0781\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0107 - mse: 0.0107 - mae: 0.0786\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0105 - mse: 0.0105 - mae: 0.0776\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0105 - mse: 0.0105 - mae: 0.0776\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0106 - mse: 0.0106 - mae: 0.0785\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0105 - mse: 0.0105 - mae: 0.0778\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - mae: 0.0779\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - mae: 0.0777\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0105 - mse: 0.0105 - mae: 0.0777\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - mae: 0.0771\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0103 - mse: 0.0103 - mae: 0.0773\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - mae: 0.0773\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - mae: 0.0777\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - mae: 0.0774\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - mae: 0.0779\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0105 - mse: 0.0105 - mae: 0.0780\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - mae: 0.0778\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0105 - mse: 0.0105 - mae: 0.0773\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 0s 894us/step - loss: 0.0104 - mse: 0.0104 - mae: 0.0772\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - mae: 0.0771\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0105 - mse: 0.0105 - mae: 0.0780\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0105 - mse: 0.0105 - mae: 0.0790\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0103 - mse: 0.0103 - mae: 0.0771\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - mae: 0.0776\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - mae: 0.0773\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0105 - mse: 0.0105 - mae: 0.0781\n"
     ]
    }
   ],
   "source": [
    "def create_seq_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Defining the input layer and first hidden layer\n",
    "    model.add(Dense(units=5, input_dim=9, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "    # Defining the second layer of the model\n",
    "    model.add(Dense(units=5, kernel_initializer='normal', activation='tanh'))\n",
    "\n",
    "    # The output neuron is a single fully connected node as only a single number is predicted\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, y_train):\n",
    "    # Fitting the ANN to the training data\n",
    "    model.fit(X_train, y_train, batch_size=20, epochs=50, verbose=1)\n",
    "\n",
    "    return model\n",
    "\n",
    "model_no = create_seq_model()\n",
    "model_no = train_model(model_no, X_train_no_trans, y_train_no_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "To find the best accuracy with the minimum number of layers/neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "Mean Square Error (MSE) is used with the formula `(sum of all (y_true - y_pred)^2)/total number of y_true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0, 0)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2221/1721340821.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mresults_no\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_no_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_no_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_no_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_no_trans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2221/1721340821.py\u001b[0m in \u001b[0;36mfind_best_params\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mMSE\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mMSE\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0, 0)"
     ]
    }
   ],
   "source": [
    "# Find the best parameters for ANN\n",
    "def find_best_params(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Defining the list of hyper parameters to try\n",
    "    batch_size_list = [5, 10, 15, 20]\n",
    "    epoch_list = [5, 10, 50, 100]\n",
    "\n",
    "    results = pd.DataFrame(columns=['trial_num', 'param', 'accuracy'])\n",
    "    \n",
    "    # Initialising the trials\n",
    "    trial_number = 0\n",
    "    for batch_size_trial in batch_size_list:\n",
    "        for epochs_trial in epoch_list:\n",
    "            trial_number += 1\n",
    "            # Create ANN model\n",
    "            model = Sequential()\n",
    "\n",
    "            # Defining the first layer of the model\n",
    "            model.add(Dense(units=5, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "            # Defining the second layer of the model\n",
    "            model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "            # Output neuron\n",
    "            model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "            # Compiling the model\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "            # Fitting the ANN to the Training set\n",
    "            model.fit(X_train, y_train ,batch_size = batch_size_trial, epochs = epochs_trial, verbose=0)\n",
    "\n",
    "            pred = model.predict(X_test)\n",
    "\n",
    "            MSE = 0\n",
    "            for i in range(y_test.shape[0]):\n",
    "                MSE += (y_test[i,0] - pred[i][0])**2\n",
    "\n",
    "            MSE /= y_test.shape[0]\n",
    "            \n",
    "            # Printing the results of the current iteration\n",
    "            print(trial_number, 'Parameters:','batch_size:', batch_size_trial,'-', 'Epochs:',epochs_trial, 'MSE:', MSE)\n",
    "            \n",
    "            results = results.append(pd.DataFrame(data=[[trial_number, str(batch_size_trial)+'-'+str(epochs_trial), MSE]], columns=['trial_num', 'param', 'mse'] ))\n",
    "\n",
    "    return(results)\n",
    "\n",
    "results_no = find_best_params(X_train_no_trans, y_train_no_trans, X_test_no_trans, y_test_no_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_bar_chart(results):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    colors = ['pink' if result != min(results['mse']) else 'purple' for result in results['mse']]\n",
    "    plt.bar(results['param'], height=results['mse'], color=colors)\n",
    "    # plt.axhline(y=1.0, color='purple', linestyle='-')\n",
    "    for a, b in zip([x for x in range(len(results['mse']))], round(results['mse'], 5)):\n",
    "        plt.text(a, b, str(b), color='black')\n",
    "    plt.show()\n",
    "\n",
    "plot_bar_chart(results_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MSE, a lower value is better. From the graph above, it can be known that the best hyperparameters are `batch_size` = 10 and `epochs` = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_data):\n",
    "    # Sum of squared error\n",
    "    test_data['SE'] = (test_data['count'] - test_data['pred_count'])**2\n",
    "    r2 = r2_score(test_data['count'], test_data['pred_count'])\n",
    "\n",
    "    print(test_data.head())\n",
    "\n",
    "    print(\"r2: \", r2)\n",
    "    print(\"MSE: \", np.mean(test_data['SE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     season      year     month  holiday   weekday   weather  temperature  \\\n",
      "0  0.449189  1.003466  0.147272 -0.16873 -1.505757 -0.736674     1.234309   \n",
      "1 -0.461798  1.003466 -0.439786 -0.16873  0.493847 -0.736674     0.533490   \n",
      "2  0.449189  1.003466  0.440802 -0.16873  0.993748  1.090784     1.475494   \n",
      "3 -0.461798  1.003466 -0.733315 -0.16873  0.993748 -0.736674    -0.504081   \n",
      "4  1.360175  1.003466  1.321389 -0.16873 -1.005856 -0.736674    -0.963703   \n",
      "\n",
      "   humidity  windspeed     count  pred_count        SE  \n",
      "0 -0.028125  -0.246866  1.095397    1.053750  0.001734  \n",
      "1 -0.761407   0.529099  1.506397    1.598219  0.008431  \n",
      "2  0.104396   0.349382  1.397250    0.881825  0.265662  \n",
      "3 -1.771513   1.460496  1.023851    0.651902  0.138346  \n",
      "4 -0.943989   0.619070  0.396644    0.270238  0.015979  \n",
      "r2:  0.8994398198769683\n",
      "MSE:  0.11479993171537962\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune hyperparameters\n",
    "# def fine_tune_model(model, batch_size, epochs, X_scaler, y_scaler, X_train_trans, y_train_trans, X_test_trans, y_test_trans, columns):\n",
    "def fine_tune_model(model, batch_size, epochs, X_train_trans, y_train_trans, X_test_trans, y_test_trans, columns):\n",
    "    model.fit(X_train_trans, y_train_trans, batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "\n",
    "    pred = model.predict(X_test_trans)\n",
    "\n",
    "    test_data_scaled = pd.DataFrame(data=X_test_trans, columns=columns)\n",
    "    test_data_scaled['count'] = y_test_trans\n",
    "    test_data_scaled['pred_count'] = pred\n",
    "    evaluate_model(test_data_scaled)\n",
    "\n",
    "    # # Scale predictions\n",
    "    # pred = y_scaler.inverse_transform(pred)\n",
    "\n",
    "    # # Scale back to original\n",
    "    # y_test = y_scaler.inverse_transform(y_test_trans)\n",
    "    # X_test = X_scaler.inverse_transform(X_test_trans)\n",
    "\n",
    "    # test_data = pd.DataFrame(data=X_test, columns=columns)\n",
    "    # test_data['count'] = y_test\n",
    "    # test_data['pred_count'] = pred\n",
    "    # print(test_data.head())\n",
    "\n",
    "    # return test_data\n",
    "    return test_data_scaled\n",
    "\n",
    "test_data_no = fine_tune_model(\n",
    "        model=model_no,\n",
    "        batch_size=10,\n",
    "        epochs=100,\n",
    "        # X_scaler=X_scaler_no,\n",
    "        # y_scaler=y_scaler_no,\n",
    "        X_train_trans=X_train_no_trans,\n",
    "        y_train_trans=y_train_no_trans,\n",
    "        X_test_trans=X_test_no_trans,\n",
    "        y_test_trans=y_test_no_trans,\n",
    "        columns=X_test_no.columns\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 5)                 50        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 30        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86\n",
      "Trainable params: 86\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_no.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Data With Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(584, 9) (584, 1) (147, 9) (147, 1)\n"
     ]
    }
   ],
   "source": [
    "X_scaler_with, y_scaler_with, X_train_with_trans, y_train_with_trans, X_test_with_trans, y_test_with_trans = standardise_data(X_train_with, y_train_with, X_test_with, y_test_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.9991 - mse: 0.9991 - mae: 0.8156\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.9879 - mse: 0.9879 - mae: 0.8109\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.9386 - mse: 0.9386 - mae: 0.7895\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.8123 - mse: 0.8123 - mae: 0.7316\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.6134 - mse: 0.6134 - mae: 0.6378\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.4300 - mse: 0.4300 - mae: 0.5299\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.3059 - mse: 0.3059 - mae: 0.4382\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.2423 - mse: 0.2423 - mae: 0.3791\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2160 - mse: 0.2160 - mae: 0.3516\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2018 - mse: 0.2018 - mae: 0.3371\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1921 - mse: 0.1921 - mae: 0.3297\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1839 - mse: 0.1839 - mae: 0.3209\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1760 - mse: 0.1760 - mae: 0.3156\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1695 - mse: 0.1695 - mae: 0.3068\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1637 - mse: 0.1637 - mae: 0.3022\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1588 - mse: 0.1588 - mae: 0.2971\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1536 - mse: 0.1536 - mae: 0.2921\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1498 - mse: 0.1498 - mae: 0.2885\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1464 - mse: 0.1464 - mae: 0.2851\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1440 - mse: 0.1440 - mae: 0.2826\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1416 - mse: 0.1416 - mae: 0.2785\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1388 - mse: 0.1388 - mae: 0.2780\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1379 - mse: 0.1379 - mae: 0.2743\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1348 - mse: 0.1348 - mae: 0.2762\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1330 - mse: 0.1330 - mae: 0.2731\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.1312 - mse: 0.1312 - mae: 0.2690\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1289 - mse: 0.1289 - mae: 0.2656\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1273 - mse: 0.1273 - mae: 0.2652\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1269 - mse: 0.1269 - mae: 0.2663\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1254 - mse: 0.1254 - mae: 0.2620\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1234 - mse: 0.1234 - mae: 0.2617\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1230 - mse: 0.1230 - mae: 0.2621\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1226 - mse: 0.1226 - mae: 0.2591\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1216 - mse: 0.1216 - mae: 0.2588\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1205 - mse: 0.1205 - mae: 0.2585\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1198 - mse: 0.1198 - mae: 0.2579\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1197 - mse: 0.1197 - mae: 0.2563\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1190 - mse: 0.1190 - mae: 0.2580\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1185 - mse: 0.1185 - mae: 0.2570\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1177 - mse: 0.1177 - mae: 0.2555\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1170 - mse: 0.1170 - mae: 0.2531\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 0s 936us/step - loss: 0.1174 - mse: 0.1174 - mae: 0.2558\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1171 - mse: 0.1171 - mae: 0.2538\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1165 - mse: 0.1165 - mae: 0.2540\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1166 - mse: 0.1166 - mae: 0.2537\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1154 - mse: 0.1154 - mae: 0.2521\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1161 - mse: 0.1161 - mae: 0.2537\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.1156 - mse: 0.1156 - mae: 0.2502\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.1148 - mse: 0.1148 - mae: 0.2521\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 0s 975us/step - loss: 0.1150 - mse: 0.1150 - mae: 0.2547\n"
     ]
    }
   ],
   "source": [
    "model_with = create_seq_model()\n",
    "model_with = train_model(model_with, X_train_with_trans, y_train_with_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same `batch_size` and `epochs` are used for accuracy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     season      year     month   holiday   weekday   weather  temperature  \\\n",
      "0  0.441688 -0.993174  0.131722 -0.167836  1.508537 -0.720359     1.060122   \n",
      "1  0.441688 -0.993174  0.131722 -0.167836  0.502271 -0.720359     1.404064   \n",
      "2 -1.362130 -0.993174 -1.603446 -0.167836  1.508537  1.141100    -1.401794   \n",
      "3 -1.362130 -0.993174 -1.314251 -0.167836 -0.503995  1.141100    -1.625316   \n",
      "4 -1.362130  1.006873 -1.603446 -0.167836  0.502271  1.141100    -0.813466   \n",
      "\n",
      "   humidity  windspeed     count  pred_count        SE  \n",
      "0 -0.276179   0.246691  0.750388    0.339964  0.168448  \n",
      "1  0.196746  -0.396460  0.050831    0.068289  0.000305  \n",
      "2 -0.891873  -0.412649 -1.706735   -1.463633  0.059098  \n",
      "3  1.469644  -1.783575 -1.647869   -1.832364  0.034038  \n",
      "4  1.041464  -1.519431 -0.220898   -0.313726  0.008617  \n",
      "r2:  0.8866752303950303\n",
      "MSE:  0.13314511075402408\n"
     ]
    }
   ],
   "source": [
    "test_data_with = fine_tune_model(\n",
    "    model=model_with,\n",
    "    batch_size=10,\n",
    "    epochs=100,\n",
    "    X_scaler=X_scaler_with,\n",
    "    y_scaler=y_scaler_with,\n",
    "    X_train_trans=X_train_with_trans,\n",
    "    y_train_trans=y_train_with_trans,\n",
    "    X_test_trans=X_test_with_trans,\n",
    "    y_test_trans=y_test_with_trans,\n",
    "    columns=X_test_with.columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_51 (Dense)            (None, 5)                 50        \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 5)                 30        \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86\n",
      "Trainable params: 86\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_with.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, using cleaned data without outliers would give a higher accuracy than using data with outliers."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
